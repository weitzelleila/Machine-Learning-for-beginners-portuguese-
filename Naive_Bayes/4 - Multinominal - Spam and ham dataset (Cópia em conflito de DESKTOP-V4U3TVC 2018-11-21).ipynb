{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h1 align=\"center\"> <span style = 'color:red'> Detecção de Spam com Multinomial NB\n",
    "</span> </h1>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* O naive bayes é muto utilizado para classificação de bases textuais\n",
    "\n",
    "\n",
    "\n",
    "* Base textuais devem ser representandas em um formato que o algoritmo possa lidar\n",
    "\n",
    "\n",
    "\n",
    "* A metodologia utilizada é processar o texto e após utilizar algum representação numérica ou modelo de linguagem\n",
    "\n",
    "\n",
    "\n",
    "*  Existem uma série de modelos de linguagem, nos proximos notebooks vamos tratar desse modelos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'> Utilizando a biblioteca NLTK para processamento de texto    </span> </h3>\n",
    "<br/>\n",
    "\n",
    "\n",
    "* A biblioteca NLTK faz toda a parte do processamento do texto, é muito bem adapatada ao idioma ingles, pouco adaptada ao idioma portugues.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 align=\"left\"> <span style = 'color:green'>Características da base </span> </h2>\n",
    "<br/>\n",
    "\n",
    "<h3 align=\"left\"> <span style = 'color:black'>  1. Contexto   </span> </h3>\n",
    "\n",
    "* The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. \n",
    "\n",
    "* It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
    "<br/>\n",
    "\n",
    "<h3 align=\"left\"> <span style = 'color:black'>  2 .Conteúdo   </span> </h3>\n",
    "\n",
    "* The files contain one message per line. \n",
    "\n",
    "\n",
    "* Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
    "\n",
    "\n",
    "* This corpus has been collected from free or free for research sources at the Internet:\n",
    "\n",
    "\n",
    "* *A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br/>     \n",
    "\n",
    "<br/>     \n",
    "\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  link para a competição no Kaggle </span> </h3>\n",
    "\n",
    "\n",
    "\n",
    "* Esse é o link para acessar a base de dados e o notebook, mas é necessário que se faça o cadastro - \"CountVectorizer and Tfidf strategies\" [Kaggle Competitions]( https://www.kaggle.com/pceccon/countvectorizer-and-tfidf-strategies/data).\n",
    "\n",
    "<br/>     \n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'> Importando bibliotecas    </span> </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  Carregando arquivo   </span> </h3>\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "* Detalhe: o idioma deste texto é o ingles logo o char-set é **latin1**, caso fosse o portugues seria **utf-8**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"spam.csv\" ,encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  Mostrando os 5 primeiros registros   </span> </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v1 é o label - rotulo a ser classificado\n",
    "# v2 é o email - texto a ser classificado em spam e ham (not-spam)\n",
    "# as outras variáveis estão com pampo vazio - NaN e devem ser retiradas\n",
    "\n",
    "\n",
    "# mostrando os 5 primeiros registros\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>   Mostrando informações  sobre a base de dados </span> </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "de2889892e8810f39628d8297c5adade4f7f121e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      "v1            5572 non-null object\n",
      "v2            5572 non-null object\n",
      "Unnamed: 2    50 non-null object\n",
      "Unnamed: 3    12 non-null object\n",
      "Unnamed: 4    6 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# informaçãoes sobre a base\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  Nomeando a entrada e saída de dados   </span> </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "c236f8a80a6d0e0b07903c1de307d38b5daaef13"
   },
   "outputs": [],
   "source": [
    "# atribuindo valores a x - email e a y - label (classe)\n",
    "\n",
    "X = data[\"v2\"].values\n",
    "y = data[\"v1\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'> Importando a biblioteca NLTK    </span> </h3>\n",
    "\n",
    "\n",
    "\n",
    "*  Atenção:\n",
    "\n",
    "\n",
    "    * Caso a bliblioeca não esteja instalada, retirar o simbolo #  rodar e instalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  Processamento do texto   </span> </h3>\n",
    "\n",
    "\n",
    "* Importando a biblioteca que faz a remoção de stopwords\n",
    "\n",
    "\n",
    "    * Stopwords são palavras que possuem alta frequencia nos documentos e em toda a coleção de documentos\n",
    "    \n",
    "    * Normalmente essas palavras não auxiliam na classifcação\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "* Importando a biblioteca regex\n",
    "\n",
    "\n",
    "    * responsável por realizar operações sobre strings no python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "79cd2ea56a5d2d0b10b2a2b560a28653c1b8ac5d"
   },
   "outputs": [],
   "source": [
    "# para retirar stopwords\n",
    "# stopword são palavras que apresentam alta frequencia em documentos e na coleção (corpus = coleção de documentos)\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# bibliotece regex para manipulação de string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> <span style = 'color:green'> Pequeno processamento do texto    </span> </h3>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "* Remoção de stopword de uma lista prévia\n",
    "\n",
    "\n",
    "* Lematizador (stemmer) separa o radical da palavra ex: trabalhador ficaria trab\n",
    "\n",
    "\n",
    "* Converte todas as palavras para minúscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "53eaef7a6e97044a609e53f7df0fb11811e56c88"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "# biblioteca para lematização, retira o sufixo das palavras\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# removendo as stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "# substituindo as palavra em maiúscula para muniúscula\n",
    "X = [re.sub('[^a-zA-Z]',' ',doc) for doc in X ]\n",
    "document = [doc.split() for doc in X ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#recriando o arquivo apenas com os stemm e sem stopword, todas as palavras em minúscula\n",
    "def convert(doc):\n",
    "    temp = []\n",
    "    for word in doc:\n",
    "        if word.lower() not in stopword:\n",
    "            temp.insert(len(temp),ps.stem(word).lower())\n",
    "    return temp        \n",
    "\n",
    "# convertendos os conjuntos de documentos (emails)\n",
    "document = [convert(doc) for doc in document] \n",
    "\n",
    "\n",
    "# convertendo em uma variável documento separada por espaços, pois a variável X que recebeu os emails tem um formato \n",
    "# que não é apropriado para ser lido pelo método countvectorizer\n",
    "document = [\" \".join(doc) for doc in document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"left\"> <span style = 'color:green'>  Método  CountVectorizer com o classificador  MultinomialNB </span> </h3>\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "* O método **Countvectorizer** é uma das formas de representar numericamente uma base textual\n",
    "\n",
    "\n",
    "    * Método baseado na frequencia que uma palavra (token = termo) tem em um documento\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* **No notebook 6  - Utilizando Countvectorize e TD-IDF para classificação de texto** tem explicação desses métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "55eec03f0c938819ec6484815cf858918a8f93a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# transformando o texto em uma representação vetorial \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Importando o classificador\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>Dividindo a base     </span> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "1141377d83c33074dd88a9e7a39663b8cf68d1b3"
   },
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(document,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  Criando modelo de linguagem e treinando   </span> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformndo o documento em vetores com no máximo 1000 características, ou seja palavras diferentes\n",
    "cv = CountVectorizer(max_features = 1000)\n",
    "\n",
    "\n",
    "\n",
    "# transformando os vetores de palavras em uma matriz de treinamento\n",
    "a = cv.fit_transform(X_train) \n",
    "\n",
    "\n",
    "# criando o vocabulario , indice de palavras\n",
    "vocab = cv.get_feature_names()\n",
    "\n",
    "\n",
    "# transformando os vetores de palavras em uma matriz de teste\n",
    "b= cv.transform(X_test) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<h3 align=\"left\"> <span style = 'color:green'>  Criando modelo de classificador e treinando   </span> </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Acurácia é de = 97.92 %\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "# criando o modelo de classificação\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# treinamento do modelo\n",
    "clf.fit(a,y_train)\n",
    "\n",
    "# mostrando o valor da acurácia\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "\n",
    "print(40 * '*')\n",
    "print('Acurácia é de =',round(clf.score(b,y_test)*100,2),'%')\n",
    "print(40 * '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
